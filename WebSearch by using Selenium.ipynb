{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Propotype with Selenium & google library\n",
    "*** The Following must using selenium with FireFox web driver. \n",
    "*** Be sure that Firefox has been installed on the local machine.\n",
    "*** The Script may only running on LOCAL MACHINE ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Library\n",
    "pip install selenium pandas google\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import time\n",
    "import traceback\n",
    "import re\n",
    "\n",
    "OUTPUT_FILE = 'output.csv'\n",
    "AUTHOR_MAX_WORDS = 3\n",
    "SLEEP_INTERVAL = 2\n",
    "\n",
    "p_elementList= []\n",
    "\n",
    "# Cuntion for Getting Date from Content\n",
    "def extractDate(string):\n",
    "    try:\n",
    "       # Define a regular expression for the date format DD/MM/YYYY and optional time\n",
    "        date_time_pattern = re.compile(\n",
    "            r\"^\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b\"  # Matches dates like DD/MM/YYYY at the start of the string\n",
    "            r\"|^\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\s+\\d{4}\\b\"  # Matches dates like December 20, 2023\n",
    "            r\"|^\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},\\d{4}\\b\"  # Matches dates like December 20, 2023\n",
    "            r\"|^\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},\\s+\\d{4}\\b\"  # Matches dates like Dec 20, 2023\n",
    "            r\"|^\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2},\\d{4}\\b\"  # Matches dates like Dec 20, 2023\n",
    "            r\"(?:\\s+\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM)?\\s*(?:ET|WT)?\\b)?$\",  # Optionally matches times with optional AM/PM and time zone\n",
    "            re.IGNORECASE  # Make the pattern case-insensitive\n",
    "        )\n",
    "        # Check if the entire string matches the pattern\n",
    "        match = date_time_pattern.match(string)\n",
    "        return bool(match)\n",
    "    except:\n",
    "        print(\"Error accessing a text:\", e)\n",
    "    return False\n",
    "\n",
    "# Function for Getting Content from url\n",
    "def getContent(driver,url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "\n",
    "        authors = '' \n",
    "        source_domain = parsed_url.netloc\n",
    "        copyright = ''\n",
    "        paragraph = ''\n",
    "        title = ''\n",
    "        date = ''\n",
    "        driver.get(url)\n",
    "        H1_elements = driver.find_elements(By.TAG_NAME, \"h1\")\n",
    "        p_elements = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "            \n",
    "        # Getting Title\n",
    "        for h1 in H1_elements:\n",
    "            try:\n",
    "                title = h1.text\n",
    "                break  # Assuming you only need the first h1's text\n",
    "            except Exception as e:\n",
    "                print(\"Error accessing h1 text:\", e)\n",
    "                continue\n",
    "\n",
    "        # Getting Content\n",
    "        for p in p_elements:\n",
    "            try:\n",
    "                value = p.text\n",
    "                # Filter out authors\n",
    "                if ((value.lower().strip().startswith('written by:')) or\\\n",
    "                    (value.lower().strip().startswith('article by:'))\n",
    "                    ):\n",
    "                    value.replace('Article by:', '')\n",
    "                    authors = value.replace('Written by:', '') \n",
    "                    #authors = value.lower().replace('by ', '') \n",
    "                    #words = authors.split()\n",
    "                    #if len(words) <= AUTHOR_MAX_WORDS and not authors:\n",
    "                    #    authors = value\n",
    "                    #else:\n",
    "                    #    authors = ''\n",
    "                    continue\n",
    "                # Filter out Copyright\n",
    "                if 'Â©' in value or 'All Rights Reserved' in value:\n",
    "                    copyright = value\n",
    "                    continue\n",
    "\n",
    "                # call date function to extract the date\n",
    "                if ((value.lower().strip().startswith('updated on')) or \\\n",
    "                    (value.lower().strip().startswith('published on')) or \\\n",
    "                    (value.lower().strip().startswith('published online')) \n",
    "                     ):\n",
    "                    value = value.replace('Updated on', '').replace(':', '') \n",
    "                    value = value.replace('Published on', '').replace(':', '') \n",
    "                    value = value.replace('Published Online', '').replace(':', '') \n",
    "                    if (extractDate(value.strip())):\n",
    "                        date = value\n",
    "                        continue\n",
    "\n",
    "                paragraph += value + ' '\n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing paragraph text from {url}:\\n\", e)\n",
    "                continue\n",
    "    except TimeoutException:\n",
    "        authors = ''\n",
    "        print(f\"Timeout waiting for elements on the page: {url}\")\n",
    "    except NoSuchElementException:\n",
    "        authors = ''\n",
    "        print(f\"An element was not found on the page: {url}\")\n",
    "    except Exception as e:\n",
    "        authors = ''\n",
    "        print(f\"An unexpected error occurred with URL => {url}: {e}\")\n",
    "\n",
    "    return {'authors': authors, 'date' : date, 'title' : title, 'content': paragraph, 'source_domain': source_domain, 'url' : url, 'copyright': copyright } \n",
    "\n",
    "def getPElementList(driver,url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "\n",
    "        authors = '' \n",
    "        source_domain = parsed_url.netloc\n",
    "        copyright = ''\n",
    "        paragraph = ''\n",
    "        title = ''\n",
    "        readMoreUrl = ''\n",
    "        date = ''\n",
    "        driver.get(url)\n",
    "        p_elements = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "\n",
    "        # Getting Content\n",
    "        for p in p_elements:\n",
    "            try:\n",
    "                value = p.text\n",
    "                p_elementList.append({'p_element': value,'title' : title,  'source_domain': source_domain, 'url' : url, })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing paragraph text from {url}:\\n\", e)\n",
    "                continue\n",
    "    except TimeoutException:\n",
    "        authors = ''\n",
    "        print(f\"Timeout waiting for elements on the page: {url}\")\n",
    "    except NoSuchElementException:\n",
    "        authors = ''\n",
    "        print(f\"An element was not found on the page: {url}\")\n",
    "    except Exception as e:\n",
    "        authors = ''\n",
    "        print(f\"An unexpected error occurred with URL => {url}: {e}\")\n",
    "\n",
    "    return \n",
    "\n",
    "def save_to_csv_with_pandas(recordSet, filename):\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(recordSet)\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run and input your query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Which query content do you want to search?\")\n",
    "queryString = input()\n",
    "\n",
    "#Testing query\n",
    "#queryString = 'Did Joe Biden tell voters to stay home and not vote?'\n",
    "#queryString = 'Is Michael Saylor saying he will give away free bitcoin?'\n",
    "#queryString = 'Is Taylor Swift going to give away a free Le Creuset cookware set?'\n",
    "#queryString = 'Did Elon Musk say you can control a computer mouse with neuralink?'\n",
    "#queryString = 'Oprah winfrey weight loss gummies?'\n",
    "#queryString = 'Did Justin Trudeau say invest in Petro canada?'\n",
    "#queryString = 'How to enter Jennifer Aniston MacBook giveaway?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run following command to get result from google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search in google url with google library\n",
    "results = search(query=queryString,num = 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run following script to get DataFrame result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = []\n",
    "driver = webdriver.Firefox()\n",
    "i = 0\n",
    "for url in results:\n",
    "    print(f'Scripting from => {url}')\n",
    "    try:\n",
    "        data = getContent(driver,url)\n",
    "        print(data)\n",
    "        if (data['title'] !='' or data['content'] != ''):\n",
    "            content = data['content']\n",
    "            if (content.strip().startswith('Log In')):\n",
    "                print(\"content Excluded!!\")\n",
    "            else:\n",
    "                dataSet.append(data)\n",
    "    except:\n",
    "        print(f\"An unexpected error occurred with URL => {url}: {e}\")\n",
    "    #time.sleep(SLEEP_INTERVAL)\n",
    "driver.close()\n",
    "df = pd.DataFrame(dataSet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export Dataframe to CSV if you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv_with_pandas(df, queryString.replace('?','.')+ 'csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
